---
title: "Design and Scale Your Own ML Platform: Lessons from Pienso"
short_description: Why Pienso chose Qdrant as a cornerstone for building domain-specific foundation models.
description: Why Pienso chose Qdrant as a cornerstone for building domain-specific foundation models.
social_preview_image: /case-studies/pienso/social_preview.png
preview_dir: /case-studies/pienso/preview
weight: 1
---

## Leveraging vector databases

Perhaps the most valuable application of vector databases is their unique capability to improve the accuracy of foundation models, or large language models (LLMs), particularly in terms of reducing hallucinations and unfounded claims generated by these models. This advantage is brought into focus when considering the relationship between vector databases and the extensive context windows managed by advanced LLMs. A vector database serves as a robust reservoir of high-dimensional vectors, mathematical representations of features, attributes or relationships laden in text data. By tapping into these vectors, LLMs produce concrete, database-backed responses, rather than leaning solely on patterns identified during the training phase. This results in more grounded, factual responses that resist the tendency for hallucination.

### Better predictions power stronger results to downstream tasks

The utility of vector databases is enhanced when combined with the large context windows of LLMs. With that extended context, an LLM can consider a broader swath of the conversation or document, making more accurate and contextually relevant predictions that deliver more powerful results to downstream tasks.

### How it works

For instance, when a model with large context windows encounters a prompt, it can scan the vector database for pertinent vectors associated with the input and its context. It identifies relevant vectors based on similarity measures such as cosine similarity, and then utilizes this information to guide the text generation process. This system blends the data-backed accuracy provided by the vector database with the contextual sensitivity offered by the large context window, ensuring more factual, contextually relevant, and consistent output, which significantly reduces the possibility of hallucinations.

## Why Pienso chose Qdrant

After a thorough evaluation of other high-performing vector databases, Pienso selected Qdrant for its best-in-class LLM interoperability. Our partnership makes it easier for commercial organizations to realize the potential of large language models and interactive deep learning, and trust the results they generate.

Qdrant differentiates itself with a unique design tailored to meet the challenges associated with storing and searching high-dimensional vectors while ensuring top-notch performance and precision. This Rust-based vector database strikes a powerful balance between high performance, flexibility, and reliability.

Compared to other vector databases, Qdrant stood out in four distinctive areas which influenced Pienso's selection:

1. **Distributed Deployment**: Qdrant supports a distributed deployment mode, which significantly increases storage capabilities and stability. In this mode, multiple Qdrant services communicate with each other to distribute data across peers. __This method of extending storage and increasing stability is a key enabler for working with massive data sets often seen in large language models and deep learning.__

2. **Efficient Storage**: Qdrant's storage efficiency is a standout feature. In our benchmark, we found that Qdrant could store 128 million documents, consuming just 20.4GB of storage and only 1.25GB of memory. __This efficiency will provide substantial hardware cost savings while ensuring that the system remains responsive, even when handling extensive data sets.__

3. **Rust**: The choice of programming language is a significant aspect when considering the performance and safety of a database. Qdrant is written in Rust, a programming language known for its speed, memory safety, and concurrency capabilities. __Rust provides Qdrant an edge in ensuring high-performance operations with robust data protection.__

4. **Memmap**: Vector databases, or vector search engines as they’re sometimes called, are designed specifically for storing and searching high-dimensional vector data, a form of data representation common in machine learning models. This focus makes them an indispensable tool for achieving efficient and rapid retrieval in systems that engage with large-scale machine learning models. __Qdrant supports memmap storage, a feature that provides fast performance comparable to in-memory storage. This capability is critical in a machine learning context where rapid data access and retrieval are required for training and inference tasks.__

## Taking it to Market 

How will building with a high-performing, open-source vector database benefit our customers and strengthen our value proposition with partners?

1. **Streamlined search speeds results that power AI return on investment:** Vector databases excel in their ability to perform efficient nearest neighbor search, a critical function in interactive deep learning. Given the complex and high-dimensional nature of data in large language models, finding similar vectors—or the "nearest neighbors"—can be a computationally expensive task. Through smart indexing and partitioning methods, vector databases greatly enhance the speed of these nearest-neighbor searches, accelerating both the training and inference process for users. 

> “Every commercial generative AI use case we encounter benefits from faster training and inference, whether mining customer interactions for next best actions or sifting clinical data to speed a therapeutic through trial and patent processes,” says Birago Jones, CEO, Pienso. 

2. **Future-Proof Your Models No Matter How Large Your Data Set Grows:** Quadrant is inherently designed for scalability. Increasing data volumes, an inevitable reality, will not compromise performance. The ability to efficiently manage growing data volumes is crucial for a database working in machine learning processes, where data not only grows exponentially and rapidly, but this data only enriches the model. 

3. **Run On Prem or In the Cloud Without Managing Another Environment:** Qdrant plays nicely on bare-metal, which means Pienso integrates Qdrant into our stack — eliminating the need for enterprise customers to manage an additional environment outside of Pienso. This benefit applies to our cloud offerings, too. Avoiding database environments that are managed outside of our platform supports data sovereignty and autonomous LLM regimes, maintaining a customer’s full span of control.

4. **Code-Level Data Persistence Enhances Data Safety and Restorability:** Thanks to Qdrant's implementation in Rust — a language celebrated for its memory safety guarantees — users can expect robust data protection. Additionally, Qdrant employs write-ahead logging (WAL), ensuring that changes are safely logged before being applied to the database, providing additional layers of data safety.

## Delivering model autonomous generative AI to enterprise users

The partnership between Pienso and Qdrant delivers a combination of no-code/low-code interactive deep learning with efficient vector computation engineered for open source models and libraries. 

> “By building on Qdrant, Pienso expands access to large language models, making interactive deep learning practical and practicable to a broad range of users.”

Bolstered by the power to swiftly and efficiently retrieve similar vectors from a high-dimensional space, Pienso's deep learning solutions powered by Qdrant ensure quick and seamless interactions for organizations training their own large language models for downstream tasks that require data sovereignty and model autonomy. 

By paving the way for a more efficient and user-friendly deep learning process, Pienso and Qdrant are bringing the deep learning experience to enterprises that want to supercharge themselves with generative and interactive AI. 

Join the founders of Pienso and Qdrant for a technical fireside chat to learn more and ask questions at 10:00 PST Tue, 25th July on Discord <link>
